---
layout: post
title: Project 2 - PCA Face Recognition
---

{{ page.title }}
============================================================

<p class="meta"/>11 Feb 2010 - St. Louis</p>

Problem Description
------------------------------------------------------------

We were allowed to chose from two projects (and a number of extra sub-projects),
however this is the one that I decided to implement:

**Face Recognition**
*This option explores the use of PCA for recognition*

1.  *Base Project, [14 points]*

    1. Acquire and make sure that you can read in the images from: the AT&T face
       database

    2. Create the PCA decomposition of this set of images (you may shrink the
       images so that this process all fits into memory), leaving out one image per
       person (for later testing).

    3. For each test image, project it onto the PCA basis and compute its coefficients.

    4. Find the training image with the most similar coefficients, and record if
       this is an image of the same person.

    5. Report recognition accuracy results for a) recognition by computing the mean
       image for each person in the database, and (for each test image) finding
       the closest mean face, and (b,c,d,e) when using 2 4 6 and 9 principle components. 

2.  *Extension, Worth [6 points]* 

    1. (This is probably *not* as easy as it seems). There are a large number of
       data sets of labelled faces. My favority is the "aligned labelled faces in
       the wild" which is drawn from faces in AP news photos, and the faces are
       then warped and aligned into a common coordinate system.

    2. Make reasonable choices for, and then report on (a) your choice of test
       and training data set -- which is not as clear because there are different
       numbers of faces for different people, (b) whether you get better performance
       when using the entire 256 x 256 image, or by using just the "central" face
       portion. Discuss your results, which results are better? Why?

    3. Discuss your results on this data set versus the data set in the base option.
       Which results are better? Why? 

Solutions
------------------------------------------------------------

What follows is a pseudo-code description of the various algorithms I used to
generate the random dot autostereograms to complete the first part of the
assignment:

TODO

Classification Accuracy
------------------------------------------------------------

TODO

Performance Results
------------------------------------------------------------

TODO

Discussion
------------------------------------------------------------

For the first implementation, I used the rough algorithm that Dr. Pless
had on the project description and quickly coded up a small script with
a few guessed numbers--it didn't work. After that I decided to rework 
the code into smaller methods and do some investigation on what kind
of separation was needed for my monitor and how that would be calculated
consistently.

After I had all the values pre-calculated, the random dot stereogram just
worked and as such the animated random dot stereogram worked as well
(since it was just creating N random dot stereograms). It did however
take a while to create and a good bit of disk space. The only problem I
faced after this portion was getting the textured stereogram working.

Since I had abstracted the separation point mapping, I was able to reuse
all the existing code and I simply had to worry with pulling the correct
pixel values from the texture.  At first this failed because my texture
was not wide enough to cover the separation needed to provide the correct
depth. After creating some pre-flight code to make sure the texture was
acceptable for the image, this portion worked as well.  The only thing that
remains is a bit of distortion in the resulting stereogram that can only
really be seen when using a texture that isn't abstract enough to hide it.

Demonstration of Successful Detections
------------------------------------------------------------

*What follows are a collection of autostereograms representing the various
required solutions for the problem sets. Open any image in a new window to
see it at full screen. It should be noted that the animated gif is 15 Mb*

**Random Dot Stereogram**

<img width="320" src="http://github.com/bashwork/school/raw/master/559/project1/images/boxes.jpg" />
<img width="320" src="http://github.com/bashwork/school/raw/master/559/project1/images/boxes-rd-sird.jpg" />

**Textured Stereogram**

<img width="320" src="http://github.com/bashwork/school/raw/master/559/project1/images/dino.jpg" />
<img width="320" src="http://github.com/bashwork/school/raw/master/559/project1/images/dino-textured-sird.jpg" />

**Animated Stereogram**

<img width="320" src="http://github.com/bashwork/school/raw/master/559/project1/images/human.gif" />
<img width="320" src="http://students.cec.wustl.edu/~gbc1/human-rd.gif" />


Code Used To Generate The Result Sets
------------------------------------------------------------

*The following is an example of creating a simple random dot stereogram*

    from stereogram import SIRD
    
    sird  = SIRD("images/some-depth-map.jpg")
    image = sird.create_random_dot()
    image.show()

*In order to create a textured stereogram, the user simply needs to supply
the texture to use to hide the depth-map*

    from stereogram import SIRD
    
    sird  = SIRD("images/some-depth-map.jpg")
    image = sird.create_texture("images/some-texture.jpg")
    image.show()

*In order to create an animated SIRD, the user first had to create
an image generator which they would then pass to the animated gif helper
library:*

    from stereogram import SIRD
    import lib
    
    sird  = SIRD("images/some-depth-map.jpg")
    images = sird.create_animated_random_dot()
    lib.CreateAnimatedGif("output.gif", images)

Complete Source Code
------------------------------------------------------------

As already mentioned, this project was implemented in python and the full
code for all the solutions can be found in the following [repository][].
As for support libraries, the following were used throughout the project:

*  [Python Imaging Library(PIL)](http://www.pythonware.com/products/pil/)

   This was used for all the low level image management and manipulation
   like opening and saving image formats and getting and setting pixel
   values

*  [Numpy](http://numpy.scipy.org/)

   This was used to operate on multi-dimensional matrices in python

*  [Scipy](http://www.scipy.org/)

   This was used for all the low level linear algebra functionality.
   Its usage allows a mostly seamless transition from Matlab to python.

Footnotes
------------------------------------------------------------

  [repository]: http://github.com/bashwork/school/tree/master/559/project1/ "Master Repository"
